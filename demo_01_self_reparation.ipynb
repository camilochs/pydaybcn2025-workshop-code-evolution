{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "69IQDTKYzVgt"
      },
      "source": [
        "# Demo 1: The Self-Reparation\n",
        "\n",
        "## Concept: Self-Correction / Reflexion\n",
        "\n",
        "In this demo, we'll explore the most basic feedback loop for self-improving code:\n",
        "\n",
        "```\n",
        "Code ‚Üí Error ‚Üí LLM ‚Üí Fixed Code\n",
        "```\n",
        "\n",
        "### What We'll Build\n",
        "\n",
        "A Python script that:\n",
        "1. Executes a buggy function\n",
        "2. Captures the error (traceback)\n",
        "3. Sends the code + error to an LLM\n",
        "4. Receives corrected code\n",
        "5. Replaces the buggy function and runs successfully\n",
        "\n",
        "### Key Insight\n",
        "\n",
        "We're using **stderr (terminal error output)** as a \"learning signal\" for the LLM. This closes the feedback loop between execution and improvement.\n",
        "\n",
        "---\n",
        "\n",
        "### Related Papers\n",
        "\n",
        "- **Self-Refine**: Iterative Refinement with Self-Feedback  \n",
        "  [arXiv:2303.17651](https://arxiv.org/abs/2303.17651)\n",
        "\n",
        "- **Reflexion**: Language Agents with Verbal Reinforcement Learning  \n",
        "  [arXiv:2303.11366](https://arxiv.org/abs/2303.11366)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7oEjJF4ZzVgu"
      },
      "source": [
        "## Setup\n",
        "\n",
        "You have three options for the LLM provider:\n",
        "\n",
        "### Option 1: OpenAI (Recommended)\n",
        "- Get an API key from [platform.openai.com](https://platform.openai.com)\n",
        "- Add secret `OPENAI_API_KEY` in Colab Secrets\n",
        "\n",
        "### Option 2: Google Gemini (FREE)\n",
        "- Get a free API key from [Google AI Studio](https://aistudio.google.com/apikey)\n",
        "- Add secret `GEMINI_API_KEY` in Colab Secrets\n",
        "\n",
        "### Option 3: Groq (FREE - Very Fast)\n",
        "- Get a free API key from [console.groq.com](https://console.groq.com)\n",
        "- Add secret `GROQ_API_KEY` in Colab Secrets\n",
        "- Uses gpt-oss model\n",
        "\n",
        "In the next cell, uncomment the option you want to use."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zexsPlCSzVgu",
        "outputId": "2c7052b7-36fb-4e71-f4a6-737fd20eef7f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Setup complete! Using OpenAI\n"
          ]
        }
      ],
      "source": [
        "# ============================================================\n",
        "# OPTION 1: OpenAI (Recommended - requires API key with credits)\n",
        "# ============================================================\n",
        "!pip install openai -q\n",
        "\n",
        "from google.colab import userdata\n",
        "from openai import OpenAI\n",
        "\n",
        "client = OpenAI(api_key=userdata.get('OPENAI_API_KEY'))\n",
        "\n",
        "print(\"Setup complete! Using OpenAI\")\n",
        "\n",
        "# ============================================================\n",
        "# OPTION 2: Google Gemini (FREE - uncomment below, comment above)\n",
        "# ============================================================\n",
        "# !pip install google-generativeai -q\n",
        "#\n",
        "# from google.colab import userdata\n",
        "# import google.generativeai as genai\n",
        "#\n",
        "# genai.configure(api_key=userdata.get('GEMINI_API_KEY'))\n",
        "#\n",
        "# # Wrapper class to make Gemini API compatible with OpenAI-style calls\n",
        "# class GeminiClient:\n",
        "#     def __init__(self):\n",
        "#         self._model = genai.GenerativeModel('gemini-1.5-flash')\n",
        "#\n",
        "#     class _Completions:\n",
        "#         def __init__(self, model):\n",
        "#             self._model = model\n",
        "#\n",
        "#         def create(self, model=None, messages=None, temperature=0.7, **kwargs):\n",
        "#             # Convert OpenAI message format to Gemini prompt\n",
        "#             prompt_parts = []\n",
        "#             for msg in messages:\n",
        "#                 role = msg.get('role', 'user')\n",
        "#                 content = msg.get('content', '')\n",
        "#                 if role == 'system':\n",
        "#                     prompt_parts.append(f\"Instructions: {content}\")\n",
        "#                 else:\n",
        "#                     prompt_parts.append(content)\n",
        "#\n",
        "#             prompt = \"\\n\\n\".join(prompt_parts)\n",
        "#\n",
        "#             response = self._model.generate_content(\n",
        "#                 prompt,\n",
        "#                 generation_config=genai.GenerationConfig(temperature=temperature)\n",
        "#             )\n",
        "#\n",
        "#             # Create OpenAI-compatible response structure\n",
        "#             class Message:\n",
        "#                 def __init__(self, text):\n",
        "#                     self.content = text\n",
        "#\n",
        "#             class Choice:\n",
        "#                 def __init__(self, text):\n",
        "#                     self.message = Message(text)\n",
        "#\n",
        "#             class Response:\n",
        "#                 def __init__(self, text):\n",
        "#                     self.choices = [Choice(text)]\n",
        "#\n",
        "#             return Response(response.text)\n",
        "#\n",
        "#     @property\n",
        "#     def chat(self):\n",
        "#         class Chat:\n",
        "#             def __init__(chat_self):\n",
        "#                 chat_self.completions = GeminiClient._Completions(self._model)\n",
        "#         return Chat()\n",
        "#\n",
        "# client = GeminiClient()\n",
        "#\n",
        "# print(\"Setup complete! Using Google Gemini (FREE)\")\n",
        "\n",
        "# ============================================================\n",
        "# OPTION 3: Groq (FREE - very fast, uncomment below, comment above)\n",
        "# ============================================================\n",
        "# !pip install openai -q\n",
        "#\n",
        "# from google.colab import userdata\n",
        "# from openai import OpenAI\n",
        "#\n",
        "# client = OpenAI(\n",
        "#     api_key=userdata.get('GROQ_API_KEY'),\n",
        "#     base_url=\"https://api.groq.com/openai/v1\"\n",
        "# )\n",
        "#\n",
        "# # IMPORTANT: When using Groq, change the model in API calls from\n",
        "# # \"gpt-4o-mini\" to \"openai/gpt-oss-20b\"\n",
        "#\n",
        "# print(\"Setup complete! Using Groq (FREE)\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8SlAbURDzVgv"
      },
      "source": [
        "## The Broken Code\n",
        "\n",
        "Here's our intentionally buggy function. It calculates the average of a list of numbers, but has a critical flaw: **it doesn't handle empty lists**.\n",
        "\n",
        "When given an empty list, it will raise a `ZeroDivisionError`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "DF_Pl1OKzVgv"
      },
      "outputs": [],
      "source": [
        "# Intentionally buggy function - division by zero when empty list\n",
        "def calculate_average(numbers):\n",
        "    \"\"\"Calculate the average of a list of numbers.\"\"\"\n",
        "    total = sum(numbers)\n",
        "    return total / len(numbers)  # Bug: fails on empty list!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dwbcna5CzVgv"
      },
      "source": [
        "## Error Capture Utility\n",
        "\n",
        "This helper function runs any function and captures both successful results and errors. The error information (including the full traceback) is what we'll feed to the LLM."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "DidRwgsqzVgv"
      },
      "outputs": [],
      "source": [
        "import traceback\n",
        "\n",
        "def run_with_error_capture(func, *args):\n",
        "    \"\"\"\n",
        "    Execute a function and capture any errors.\n",
        "\n",
        "    Returns a dict with:\n",
        "    - success: bool\n",
        "    - result: the return value (if success)\n",
        "    - error_type, error_message, traceback: error details (if failure)\n",
        "    \"\"\"\n",
        "    try:\n",
        "        result = func(*args)\n",
        "        return {\"success\": True, \"result\": result}\n",
        "    except Exception as e:\n",
        "        return {\n",
        "            \"success\": False,\n",
        "            \"error_type\": type(e).__name__,\n",
        "            \"error_message\": str(e),\n",
        "            \"traceback\": traceback.format_exc()\n",
        "        }"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VrFWUN-HzVgv"
      },
      "source": [
        "## Watch It Fail\n",
        "\n",
        "Let's run our function with two test cases:\n",
        "1. A normal list `[1, 2, 3, 4, 5]` ‚Üí should return `3.0`\n",
        "2. An empty list `[]` ‚Üí will trigger the bug!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Mwgh0Dr9zVgw",
        "outputId": "b545deda-81d6-4ab3-bebf-d0e71279bbe5"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "üß™ Running tests on the buggy function:\n",
            "\n",
            "Input: [19, 92, 20, 25]\n",
            "‚úÖ Result: 39.0\n",
            "\n",
            "Input: []\n",
            "‚ùå Error: ZeroDivisionError: division by zero\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# Test cases\n",
        "# \"To err is human, to self-repair is divine\" - Workshop 2025\n",
        "test_cases = [\n",
        "    [19, 92, 20, 25],  # Normal case: should return 39.0\n",
        "    [],                # Edge case: empty list (bug trigger!)\n",
        "]\n",
        "\n",
        "print(\"üß™ Running tests on the buggy function:\\n\")\n",
        "\n",
        "for test in test_cases:\n",
        "    result = run_with_error_capture(calculate_average, test)\n",
        "    print(f\"Input: {test}\")\n",
        "    if result[\"success\"]:\n",
        "        print(f\"‚úÖ Result: {result['result']}\")\n",
        "    else:\n",
        "        print(f\"‚ùå Error: {result['error_type']}: {result['error_message']}\")\n",
        "    print()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "O5gBooYzzVgw"
      },
      "source": [
        "## The Self-Repairing Loop\n",
        "\n",
        "Now for the magic! We'll create a function that:\n",
        "1. Takes broken code and an error message\n",
        "2. Sends them to the LLM with a prompt asking for a fix\n",
        "3. Extracts and returns the corrected code\n",
        "\n",
        "This is the **core of the self-repairing pattern**: using execution feedback to guide improvement."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "VS8pqiikzVgw"
      },
      "outputs": [],
      "source": [
        "def ask_llm_to_fix(code: str, error: str) -> str:\n",
        "    \"\"\"\n",
        "    Send broken code + error to LLM, get fixed code back.\n",
        "\n",
        "    Args:\n",
        "        code: The source code of the buggy function\n",
        "        error: The full traceback from the error\n",
        "\n",
        "    Returns:\n",
        "        The corrected Python code as a string\n",
        "    \"\"\"\n",
        "\n",
        "    prompt = f\"\"\"Here is a Python function that has a bug:\n",
        "\n",
        "    ```python\n",
        "    {code}\n",
        "    ```\n",
        "\n",
        "    When executed, it produces this error:\n",
        "    ```\n",
        "    {error}\n",
        "    ```\n",
        "\n",
        "    Please fix the bug and return ONLY the corrected Python code block, nothing else.\n",
        "    Make sure to handle edge cases appropriately.\n",
        "    \"\"\"\n",
        "\n",
        "    response = client.chat.completions.create(\n",
        "        model=\"gpt-4o-mini\",\n",
        "        messages=[{\"role\": \"user\", \"content\": prompt}],\n",
        "        temperature=0  # More stable output for reproducibility\n",
        "    )\n",
        "\n",
        "    # Extract code from response\n",
        "    content = response.choices[0].message.content\n",
        "\n",
        "    # Parse code block (handle different markdown formats)\n",
        "    if \"```python\" in content:\n",
        "        code = content.split(\"```python\")[1].split(\"```\")[0]\n",
        "    elif \"```\" in content:\n",
        "        code = content.split(\"```\")[1].split(\"```\")[0]\n",
        "    else:\n",
        "        code = content\n",
        "\n",
        "    return code.strip()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "H8zCbDvEzVgw"
      },
      "source": [
        "## Running the Self-Repairing Process\n",
        "\n",
        "Here's where everything comes together:\n",
        "\n",
        "1. We get the source code of our buggy function\n",
        "2. We run it and capture the error\n",
        "3. We ask the LLM to fix it\n",
        "4. We execute the fixed code to redefine the function\n",
        "5. We test again to verify the fix works!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DmkqQlFQzVgw",
        "outputId": "c6b8c10e-7a5e-4809-91f5-d4ff66215215"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "üìÑ Original code:\n",
            "def calculate_average(numbers):\n",
            "    \"\"\"Calculate the average of a list of numbers.\"\"\"\n",
            "    total = sum(numbers)\n",
            "    return total / len(numbers)  # Bug: fails on empty list!\n",
            "\n",
            "\n",
            "üî¥ Error detected!\n",
            "Error: ZeroDivisionError: division by zero\n",
            "\n",
            "üîß Asking LLM to fix the code...\n",
            "\n",
            "‚úÖ Fixed code received:\n",
            "----------------------------------------\n",
            "def calculate_average(numbers):\n",
            "    \"\"\"Calculate the average of a list of numbers.\"\"\"\n",
            "    if not numbers:  # Check for empty list\n",
            "        return 0  # Return 0 or an appropriate value for empty list\n",
            "    total = sum(numbers)\n",
            "    return total / len(numbers)\n",
            "----------------------------------------\n",
            "\n",
            "üß™ Testing the fixed function:\n",
            "\n",
            "Input: [19, 92, 20, 25]\n",
            "‚úÖ Result: 39.0\n",
            "\n",
            "Input: []\n",
            "‚úÖ Result: 0\n",
            "\n"
          ]
        }
      ],
      "source": [
        "import inspect\n",
        "\n",
        "# Get the source code of our buggy function\n",
        "original_code = inspect.getsource(calculate_average)\n",
        "print(\"üìÑ Original code:\")\n",
        "print(original_code)\n",
        "print()\n",
        "\n",
        "# Run and capture error\n",
        "result = run_with_error_capture(calculate_average, [])\n",
        "\n",
        "if not result[\"success\"]:\n",
        "    print(\"üî¥ Error detected!\")\n",
        "    print(f\"Error: {result['error_type']}: {result['error_message']}\\n\")\n",
        "\n",
        "    print(\"üîß Asking LLM to fix the code...\\n\")\n",
        "    fixed_code = ask_llm_to_fix(original_code, result[\"traceback\"])\n",
        "\n",
        "    print(\"‚úÖ Fixed code received:\")\n",
        "    print(\"-\" * 40)\n",
        "    print(fixed_code)\n",
        "    print(\"-\" * 40)\n",
        "\n",
        "    # Execute the fixed code to define the new function\n",
        "    # This replaces the old calculate_average with the fixed version\n",
        "    exec(fixed_code, globals())\n",
        "\n",
        "    print(\"\\nüß™ Testing the fixed function:\")\n",
        "    print()\n",
        "\n",
        "    for test in test_cases:\n",
        "        result = run_with_error_capture(calculate_average, test)\n",
        "        print(f\"Input: {test}\")\n",
        "        if result[\"success\"]:\n",
        "            print(f\"‚úÖ Result: {result['result']}\")\n",
        "        else:\n",
        "            print(f\"‚ùå Still broken: {result['error_message']}\")\n",
        "        print()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MmanwYnzzVgw"
      },
      "source": [
        "## Key Takeaways\n",
        "\n",
        "### What We Learned\n",
        "\n",
        "1. **The Feedback Loop**: By capturing execution errors and feeding them back to an LLM, we create a simple but powerful self-correction mechanism.\n",
        "\n",
        "2. **stderr as Learning Signal**: The traceback isn't just for humans - it's rich information that tells the LLM exactly what went wrong and where.\n",
        "\n",
        "3. **Dynamic Code Replacement**: Using `exec()` we can replace functions at runtime with their corrected versions.\n",
        "\n",
        "### Limitations & Safety Considerations\n",
        "\n",
        "‚ö†Ô∏è‚ö†Ô∏è‚ö†Ô∏è **This is a demo!** In production:\n",
        "\n",
        "- **Never `exec()` untrusted code** - always sandbox LLM-generated code\n",
        "- **Validate fixes** - run comprehensive tests before accepting changes\n",
        "- **Human review** - critical changes should be reviewed before deployment\n",
        "- **Rate limiting** - prevent infinite loops of failed fixes\n",
        "\n",
        "### Next Steps\n",
        "\n",
        "In Demo 2, we'll expand this concept to **evolutionary optimization** - instead of fixing one bug, we'll evolve an entire population of solutions!"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}