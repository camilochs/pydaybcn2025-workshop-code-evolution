{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Demo 4: The Evolving Dev Team\n",
    "\n",
    "## Concept: Self-Evolving Agent Prompts\n",
    "\n",
    "In this advanced demo, we build a team of AI developers that **evolve their own system prompts** to become better programmers. This is meta-learning: agents that improve their own \"personality\" and strategies through competition and self-reflection.\n",
    "\n",
    "### What We'll See\n",
    "\n",
    "1. Two developer agents with different philosophies compete to solve coding problems\n",
    "2. A Tech Lead evaluates solutions by actually running tests\n",
    "3. After feedback, each agent rewrites their own system prompt\n",
    "4. In Round 2, we see if their evolved prompts lead to better performance\n",
    "\n",
    "### The Key Insight\n",
    "\n",
    "The agents don't just solve problems - they reflect on their performance and rewrite their own instructions. We'll observe:\n",
    "- Self-reflection capabilities of LLMs\n",
    "- Prompt optimization through competition\n",
    "- Emergent strategies we didn't explicitly program\n",
    "\n",
    "---\n",
    "\n",
    "### Related Papers\n",
    "\n",
    "- **Self-Refine**: Iterative Refinement with Self-Feedback  \n",
    "  [arXiv:2303.17651](https://arxiv.org/abs/2303.17651)\n",
    "\n",
    "- **OPRO**: Large Language Models as Optimizers  \n",
    "  [arXiv:2309.03409](https://arxiv.org/abs/2309.03409)\n",
    "\n",
    "- **Constitutional AI**: Harmlessness from AI Feedback  \n",
    "  [arXiv:2212.08073](https://arxiv.org/abs/2212.08073)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup\n",
    "\n",
    "You have three options for the LLM provider:\n",
    "\n",
    "### Option 1: OpenAI (Recommended)\n",
    "1. Get an API key from [platform.openai.com](https://platform.openai.com)\n",
    "2. Add secret `OPENAI_API_KEY` in Colab Secrets (key icon in sidebar)\n",
    "\n",
    "### Option 2: Google Gemini (FREE)\n",
    "1. Get a free API key from [Google AI Studio](https://aistudio.google.com/apikey)\n",
    "2. Add secret `GEMINI_API_KEY` in Colab Secrets\n",
    "3. In the next cell, comment out the OpenAI section and uncomment the Gemini section\n",
    "\n",
    "### Option 3: Groq (FREE - Very Fast)\n",
    "1. Get a free API key from [console.groq.com](https://console.groq.com)\n",
    "2. Add secret `GROQ_API_KEY` in Colab Secrets\n",
    "3. In the next cell, comment out the OpenAI section and uncomment the Groq section\n",
    "4. Uses Llama 3.1 model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# OPTION 1: OpenAI (Recommended - requires API key with credits)\n",
    "# ============================================================\n",
    "!pip install openai -q\n",
    "\n",
    "from google.colab import userdata\n",
    "from openai import OpenAI\n",
    "import time\n",
    "import traceback\n",
    "from typing import Callable\n",
    "from functools import wraps\n",
    "\n",
    "client = OpenAI(api_key=userdata.get('OPENAI_API_KEY'))\n",
    "\n",
    "print(\"Setup complete! Using OpenAI\")\n",
    "\n",
    "# ============================================================\n",
    "# OPTION 2: Google Gemini (FREE - uncomment below, comment above)\n",
    "# ============================================================\n",
    "# !pip install google-generativeai -q\n",
    "#\n",
    "# from google.colab import userdata\n",
    "# import google.generativeai as genai\n",
    "# import time\n",
    "# import traceback\n",
    "# from typing import Callable\n",
    "# from functools import wraps\n",
    "#\n",
    "# genai.configure(api_key=userdata.get('GEMINI_API_KEY'))\n",
    "#\n",
    "# # Wrapper class to make Gemini API compatible with OpenAI-style calls\n",
    "# class GeminiClient:\n",
    "#     def __init__(self):\n",
    "#         self._model = genai.GenerativeModel('gemini-1.5-flash')\n",
    "#\n",
    "#     class _Completions:\n",
    "#         def __init__(self, model):\n",
    "#             self._model = model\n",
    "#\n",
    "#         def create(self, model=None, messages=None, temperature=0.7, **kwargs):\n",
    "#             prompt_parts = []\n",
    "#             for msg in messages:\n",
    "#                 role = msg.get('role', 'user')\n",
    "#                 content = msg.get('content', '')\n",
    "#                 if role == 'system':\n",
    "#                     prompt_parts.append(f\"Instructions: {content}\")\n",
    "#                 else:\n",
    "#                     prompt_parts.append(content)\n",
    "#\n",
    "#             prompt = \"\\n\\n\".join(prompt_parts)\n",
    "#\n",
    "#             response = self._model.generate_content(\n",
    "#                 prompt,\n",
    "#                 generation_config=genai.GenerationConfig(temperature=temperature)\n",
    "#             )\n",
    "#\n",
    "#             class Message:\n",
    "#                 def __init__(self, text):\n",
    "#                     self.content = text\n",
    "#\n",
    "#             class Choice:\n",
    "#                 def __init__(self, text):\n",
    "#                     self.message = Message(text)\n",
    "#\n",
    "#             class Response:\n",
    "#                 def __init__(self, text):\n",
    "#                     self.choices = [Choice(text)]\n",
    "#\n",
    "#             return Response(response.text)\n",
    "#\n",
    "#     @property\n",
    "#     def chat(self):\n",
    "#         class Chat:\n",
    "#             def __init__(chat_self):\n",
    "#                 chat_self.completions = GeminiClient._Completions(self._model)\n",
    "#         return Chat()\n",
    "#\n",
    "# client = GeminiClient()\n",
    "#\n",
    "# print(\"Setup complete! Using Google Gemini (FREE)\")\n",
    "\n",
    "# ============================================================\n",
    "# OPTION 3: Groq (FREE - very fast, uncomment below, comment above)\n",
    "# ============================================================\n",
    "# !pip install openai -q\n",
    "#\n",
    "# from google.colab import userdata\n",
    "# from openai import OpenAI\n",
    "# import time\n",
    "# import traceback\n",
    "# from typing import Callable\n",
    "# from functools import wraps\n",
    "#\n",
    "# client = OpenAI(\n",
    "#     api_key=userdata.get('GROQ_API_KEY'),\n",
    "#     base_url=\"https://api.groq.com/openai/v1\"\n",
    "# )\n",
    "#\n",
    "# # IMPORTANT: When using Groq, change the model in API calls from\n",
    "# # \"gpt-4o-mini\" to \"openai/gpt-oss-20b\"\n",
    "#\n",
    "# print(\"Setup complete! Using Groq (FREE)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The Coding Problems\n",
    "\n",
    "We define two problems with test cases. The agents will solve these, and their solutions will be actually executed to verify correctness."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Problem 1: Find pairs that sum to a target\n",
    "PROBLEM_1 = {\n",
    "    \"name\": \"find_pairs\",\n",
    "    \"description\": \"\"\"Write a Python function called `find_pairs` that finds all unique pairs of numbers in a list that sum to a target value.\n",
    "\n",
    "Function signature:\n",
    "def find_pairs(numbers: list[int], target: int) -> list[tuple[int, int]]\n",
    "\n",
    "Requirements:\n",
    "- Return a list of tuples, each containing two numbers that sum to target\n",
    "- Each pair should only appear once (no duplicates)\n",
    "- The smaller number should come first in each tuple\n",
    "- Handle edge cases: empty list, no valid pairs, duplicate numbers\n",
    "\n",
    "Examples:\n",
    "- find_pairs([1, 2, 3, 4, 5], 6) -> [(1, 5), (2, 4)]\n",
    "- find_pairs([1, 1, 2, 3], 4) -> [(1, 3)]\n",
    "- find_pairs([], 5) -> []\n",
    "\"\"\",\n",
    "    \"test_cases\": [\n",
    "        {\n",
    "            \"input\": {\"numbers\": [1, 2, 3, 4, 5], \"target\": 6},\n",
    "            \"expected\": [(1, 5), (2, 4)],\n",
    "            \"name\": \"basic case\"\n",
    "        },\n",
    "        {\n",
    "            \"input\": {\"numbers\": [1, 1, 2, 3], \"target\": 4},\n",
    "            \"expected\": [(1, 3)],\n",
    "            \"name\": \"with duplicates\"\n",
    "        },\n",
    "        {\n",
    "            \"input\": {\"numbers\": [], \"target\": 5},\n",
    "            \"expected\": [],\n",
    "            \"name\": \"empty list\"\n",
    "        },\n",
    "        {\n",
    "            \"input\": {\"numbers\": [1, 2, 3], \"target\": 10},\n",
    "            \"expected\": [],\n",
    "            \"name\": \"no valid pairs\"\n",
    "        },\n",
    "    ]\n",
    "}\n",
    "\n",
    "# Problem 2: Retry decorator\n",
    "PROBLEM_2 = {\n",
    "    \"name\": \"retry\",\n",
    "    \"description\": \"\"\"Write a Python decorator called `retry` that retries a function when it raises an exception.\n",
    "\n",
    "Function signature:\n",
    "def retry(max_attempts: int = 3, delay: float = 0.1)\n",
    "\n",
    "Requirements:\n",
    "- The decorator should retry the function up to max_attempts times\n",
    "- It should wait delay seconds between each retry\n",
    "- If the function succeeds, return its result immediately\n",
    "- If all attempts fail, raise the last exception\n",
    "- The decorator should work with functions that have any arguments\n",
    "\n",
    "Example usage:\n",
    "@retry(max_attempts=3, delay=0.1)\n",
    "def flaky_function():\n",
    "    # might fail sometimes\n",
    "    pass\n",
    "\"\"\",\n",
    "    \"test_cases\": []  # We'll use custom test logic for the decorator\n",
    "}\n",
    "\n",
    "print(\"Problems defined!\")\n",
    "print(f\"Problem 1: {PROBLEM_1['name']}\")\n",
    "print(f\"Problem 2: {PROBLEM_2['name']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The Developer Agent\n",
    "\n",
    "Each developer has:\n",
    "- A name and initial system prompt (their \"personality\")\n",
    "- The ability to solve problems\n",
    "- The ability to evolve their own prompt based on feedback"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Developer:\n",
    "    \"\"\"\n",
    "    An AI developer agent with an evolvable system prompt.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, name: str, initial_prompt: str):\n",
    "        self.name = name\n",
    "        self.system_prompt = initial_prompt\n",
    "        self.prompt_history = [initial_prompt]\n",
    "        self.score_history = []\n",
    "        self.solutions = []\n",
    "\n",
    "    def solve(self, problem: dict) -> str:\n",
    "        \"\"\"\n",
    "        Generate a solution for the given problem using current system prompt.\n",
    "\n",
    "        Returns the code as a string.\n",
    "        \"\"\"\n",
    "        user_message = f\"\"\"Solve this programming problem:\n",
    "\n",
    "{problem['description']}\n",
    "\n",
    "Return ONLY the Python code, no explanations or markdown.\"\"\"\n",
    "\n",
    "        response = client.chat.completions.create(\n",
    "            model=\"gpt-4o-mini\",\n",
    "            messages=[\n",
    "                {\"role\": \"system\", \"content\": self.system_prompt},\n",
    "                {\"role\": \"user\", \"content\": user_message}\n",
    "            ],\n",
    "            temperature=0.7\n",
    "        )\n",
    "\n",
    "        code = response.choices[0].message.content\n",
    "\n",
    "        # Clean up code block markers if present\n",
    "        if \"```python\" in code:\n",
    "            code = code.split(\"```python\")[1].split(\"```\")[0]\n",
    "        elif \"```\" in code:\n",
    "            code = code.split(\"```\")[1].split(\"```\")[0]\n",
    "\n",
    "        code = code.strip()\n",
    "        self.solutions.append(code)\n",
    "        return code\n",
    "\n",
    "    def evolve_prompt(self, feedback: str, my_score: int,\n",
    "                      opponent_score: int, winning_code: str = None):\n",
    "        \"\"\"\n",
    "        Rewrite own system prompt based on feedback.\n",
    "        This is the self-evolution mechanism!\n",
    "        \"\"\"\n",
    "        evolution_message = f\"\"\"You are an AI developer agent reflecting on your performance.\n",
    "\n",
    "Your current system prompt is:\n",
    "---\n",
    "{self.system_prompt}\n",
    "---\n",
    "\n",
    "In the last coding challenge:\n",
    "- Your score: {my_score}/100\n",
    "- Opponent's score: {opponent_score}/100\n",
    "- Tech Lead feedback: {feedback}\n",
    "\"\"\"\n",
    "\n",
    "        if winning_code and opponent_score > my_score:\n",
    "            evolution_message += f\"\"\"\\n- The winning solution was:\n",
    "```python\n",
    "{winning_code}\n",
    "```\n",
    "\"\"\"\n",
    "\n",
    "        evolution_message += \"\"\"\n",
    "Based on this feedback, rewrite your system prompt to perform better in future coding challenges.\n",
    "\n",
    "Guidelines:\n",
    "- Focus on specific, actionable improvements\n",
    "- Learn from what worked in the winning solution (if you lost)\n",
    "- Keep the prompt concise (under 150 words)\n",
    "- Maintain your core identity but address weaknesses\n",
    "\n",
    "Return ONLY the new system prompt text, nothing else.\"\"\"\n",
    "\n",
    "        response = client.chat.completions.create(\n",
    "            model=\"gpt-4o-mini\",\n",
    "            messages=[{\"role\": \"user\", \"content\": evolution_message}],\n",
    "            temperature=0.7\n",
    "        )\n",
    "\n",
    "        new_prompt = response.choices[0].message.content.strip()\n",
    "\n",
    "        # Remove quotes if the model wrapped the prompt in them\n",
    "        if new_prompt.startswith('\"') and new_prompt.endswith('\"'):\n",
    "            new_prompt = new_prompt[1:-1]\n",
    "\n",
    "        self.system_prompt = new_prompt\n",
    "        self.prompt_history.append(new_prompt)\n",
    "\n",
    "        return new_prompt\n",
    "\n",
    "print(\"Developer class defined!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The Tech Lead (Evaluator)\n",
    "\n",
    "The Tech Lead:\n",
    "- Executes submitted code against test cases\n",
    "- Scores solutions based on correctness, style, and other criteria\n",
    "- Provides actionable feedback"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TechLead:\n",
    "    \"\"\"\n",
    "    Evaluates developer solutions by running tests and providing scores.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self):\n",
    "        self.criteria = [\n",
    "            (\"correctness\", 40),\n",
    "            (\"performance\", 20),\n",
    "            (\"readability\", 20),\n",
    "            (\"pythonic\", 10),\n",
    "            (\"edge_cases\", 10)\n",
    "        ]\n",
    "\n",
    "    def run_tests_problem1(self, code: str, test_cases: list) -> dict:\n",
    "        \"\"\"\n",
    "        Execute code for Problem 1 (find_pairs) and run test cases.\n",
    "        \"\"\"\n",
    "        results = {\n",
    "            \"passed\": 0,\n",
    "            \"failed\": 0,\n",
    "            \"errors\": [],\n",
    "            \"details\": []\n",
    "        }\n",
    "\n",
    "        # Create execution namespace\n",
    "        namespace = {}\n",
    "\n",
    "        try:\n",
    "            exec(code, namespace)\n",
    "        except Exception as e:\n",
    "            results[\"errors\"].append(f\"Code compilation error: {e}\")\n",
    "            return results\n",
    "\n",
    "        if \"find_pairs\" not in namespace:\n",
    "            results[\"errors\"].append(\"Function 'find_pairs' not found\")\n",
    "            return results\n",
    "\n",
    "        func = namespace[\"find_pairs\"]\n",
    "\n",
    "        for tc in test_cases:\n",
    "            try:\n",
    "                result = func(**tc[\"input\"])\n",
    "                # Normalize result for comparison (sort tuples and list)\n",
    "                result_normalized = sorted([tuple(sorted(p)) for p in result])\n",
    "                expected_normalized = sorted([tuple(sorted(p)) for p in tc[\"expected\"]])\n",
    "\n",
    "                if result_normalized == expected_normalized:\n",
    "                    results[\"passed\"] += 1\n",
    "                    results[\"details\"].append(f\"PASS: {tc['name']}\")\n",
    "                else:\n",
    "                    results[\"failed\"] += 1\n",
    "                    results[\"details\"].append(\n",
    "                        f\"FAIL: {tc['name']} - Expected {tc['expected']}, got {result}\"\n",
    "                    )\n",
    "            except Exception as e:\n",
    "                results[\"failed\"] += 1\n",
    "                results[\"errors\"].append(f\"Runtime error in {tc['name']}: {e}\")\n",
    "\n",
    "        return results\n",
    "\n",
    "    def run_tests_problem2(self, code: str) -> dict:\n",
    "        \"\"\"\n",
    "        Execute code for Problem 2 (retry decorator) and run test cases.\n",
    "        \"\"\"\n",
    "        results = {\n",
    "            \"passed\": 0,\n",
    "            \"failed\": 0,\n",
    "            \"errors\": [],\n",
    "            \"details\": []\n",
    "        }\n",
    "\n",
    "        # Create execution namespace with time module\n",
    "        namespace = {\"time\": time, \"wraps\": wraps}\n",
    "\n",
    "        try:\n",
    "            exec(code, namespace)\n",
    "        except Exception as e:\n",
    "            results[\"errors\"].append(f\"Code compilation error: {e}\")\n",
    "            return results\n",
    "\n",
    "        if \"retry\" not in namespace:\n",
    "            results[\"errors\"].append(\"Decorator 'retry' not found\")\n",
    "            return results\n",
    "\n",
    "        retry_decorator = namespace[\"retry\"]\n",
    "\n",
    "        # Test 1: Function that succeeds immediately\n",
    "        try:\n",
    "            @retry_decorator(max_attempts=3, delay=0.01)\n",
    "            def always_works():\n",
    "                return \"success\"\n",
    "\n",
    "            if always_works() == \"success\":\n",
    "                results[\"passed\"] += 1\n",
    "                results[\"details\"].append(\"PASS: Function that succeeds immediately\")\n",
    "            else:\n",
    "                results[\"failed\"] += 1\n",
    "                results[\"details\"].append(\"FAIL: Wrong return value for successful function\")\n",
    "        except Exception as e:\n",
    "            results[\"failed\"] += 1\n",
    "            results[\"errors\"].append(f\"Test 1 error: {e}\")\n",
    "\n",
    "        # Test 2: Function that fails then succeeds\n",
    "        try:\n",
    "            call_count = [0]\n",
    "\n",
    "            @retry_decorator(max_attempts=3, delay=0.01)\n",
    "            def fails_twice():\n",
    "                call_count[0] += 1\n",
    "                if call_count[0] < 3:\n",
    "                    raise ValueError(\"Not yet!\")\n",
    "                return \"finally!\"\n",
    "\n",
    "            result = fails_twice()\n",
    "            if result == \"finally!\" and call_count[0] == 3:\n",
    "                results[\"passed\"] += 1\n",
    "                results[\"details\"].append(\"PASS: Function that fails then succeeds\")\n",
    "            else:\n",
    "                results[\"failed\"] += 1\n",
    "                results[\"details\"].append(f\"FAIL: Expected 3 calls, got {call_count[0]}\")\n",
    "        except Exception as e:\n",
    "            results[\"failed\"] += 1\n",
    "            results[\"errors\"].append(f\"Test 2 error: {e}\")\n",
    "\n",
    "        # Test 3: Function that always fails\n",
    "        try:\n",
    "            @retry_decorator(max_attempts=2, delay=0.01)\n",
    "            def always_fails():\n",
    "                raise RuntimeError(\"I always fail!\")\n",
    "\n",
    "            try:\n",
    "                always_fails()\n",
    "                results[\"failed\"] += 1\n",
    "                results[\"details\"].append(\"FAIL: Should have raised exception\")\n",
    "            except RuntimeError:\n",
    "                results[\"passed\"] += 1\n",
    "                results[\"details\"].append(\"PASS: Raises exception after max attempts\")\n",
    "        except Exception as e:\n",
    "            results[\"failed\"] += 1\n",
    "            results[\"errors\"].append(f\"Test 3 error: {e}\")\n",
    "\n",
    "        # Test 4: Preserves function arguments\n",
    "        try:\n",
    "            @retry_decorator(max_attempts=2, delay=0.01)\n",
    "            def add(a, b):\n",
    "                return a + b\n",
    "\n",
    "            if add(2, 3) == 5:\n",
    "                results[\"passed\"] += 1\n",
    "                results[\"details\"].append(\"PASS: Preserves function arguments\")\n",
    "            else:\n",
    "                results[\"failed\"] += 1\n",
    "                results[\"details\"].append(\"FAIL: Wrong result with arguments\")\n",
    "        except Exception as e:\n",
    "            results[\"failed\"] += 1\n",
    "            results[\"errors\"].append(f\"Test 4 error: {e}\")\n",
    "\n",
    "        return results\n",
    "\n",
    "    def evaluate(self, code: str, problem: dict) -> dict:\n",
    "        \"\"\"\n",
    "        Full evaluation: run tests and score based on multiple criteria.\n",
    "        \"\"\"\n",
    "        # Run appropriate tests\n",
    "        if problem[\"name\"] == \"find_pairs\":\n",
    "            test_results = self.run_tests_problem1(code, problem[\"test_cases\"])\n",
    "            total_tests = len(problem[\"test_cases\"])\n",
    "        else:  # retry decorator\n",
    "            test_results = self.run_tests_problem2(code)\n",
    "            total_tests = 4\n",
    "\n",
    "        # Calculate scores\n",
    "        scores = {}\n",
    "\n",
    "        # Correctness (40 pts) - based on test pass rate\n",
    "        if total_tests > 0:\n",
    "            pass_rate = test_results[\"passed\"] / total_tests\n",
    "            scores[\"correctness\"] = int(40 * pass_rate)\n",
    "        else:\n",
    "            scores[\"correctness\"] = 0\n",
    "\n",
    "        # Use LLM to evaluate other criteria\n",
    "        style_prompt = f\"\"\"Evaluate this Python code on a scale of 0-10 for each criterion.\n",
    "\n",
    "Code:\n",
    "```python\n",
    "{code}\n",
    "```\n",
    "\n",
    "Criteria:\n",
    "1. Performance (time/space complexity, efficient algorithms)\n",
    "2. Readability (clear variable names, good structure, comments if needed)\n",
    "3. Pythonic (uses Python idioms, list comprehensions where appropriate, type hints)\n",
    "4. Edge Cases (handles empty inputs, None values, boundary conditions)\n",
    "\n",
    "Return ONLY four numbers separated by commas, nothing else.\n",
    "Example: 8,7,6,9\"\"\"\n",
    "\n",
    "        try:\n",
    "            response = client.chat.completions.create(\n",
    "                model=\"gpt-4o-mini\",\n",
    "                messages=[{\"role\": \"user\", \"content\": style_prompt}],\n",
    "                temperature=0\n",
    "            )\n",
    "\n",
    "            style_scores = response.choices[0].message.content.strip()\n",
    "            parts = [int(x.strip()) for x in style_scores.split(\",\")]\n",
    "\n",
    "            scores[\"performance\"] = min(20, parts[0] * 2)\n",
    "            scores[\"readability\"] = min(20, parts[1] * 2)\n",
    "            scores[\"pythonic\"] = min(10, parts[2])\n",
    "            scores[\"edge_cases\"] = min(10, parts[3])\n",
    "        except:\n",
    "            # Fallback scores if LLM evaluation fails\n",
    "            scores[\"performance\"] = 10\n",
    "            scores[\"readability\"] = 10\n",
    "            scores[\"pythonic\"] = 5\n",
    "            scores[\"edge_cases\"] = 5\n",
    "\n",
    "        total_score = sum(scores.values())\n",
    "\n",
    "        return {\n",
    "            \"scores\": scores,\n",
    "            \"total\": total_score,\n",
    "            \"test_results\": test_results\n",
    "        }\n",
    "\n",
    "    def generate_feedback(self, dev_name: str, evaluation: dict, code: str) -> str:\n",
    "        \"\"\"\n",
    "        Generate actionable feedback for a developer.\n",
    "        \"\"\"\n",
    "        prompt = f\"\"\"As a Tech Lead, provide brief, actionable feedback for {dev_name}.\n",
    "\n",
    "Their code:\n",
    "```python\n",
    "{code}\n",
    "```\n",
    "\n",
    "Scores:\n",
    "- Correctness: {evaluation['scores']['correctness']}/40\n",
    "- Performance: {evaluation['scores']['performance']}/20\n",
    "- Readability: {evaluation['scores']['readability']}/20\n",
    "- Pythonic: {evaluation['scores']['pythonic']}/10\n",
    "- Edge Cases: {evaluation['scores']['edge_cases']}/10\n",
    "- Total: {evaluation['total']}/100\n",
    "\n",
    "Test results: {evaluation['test_results']['passed']} passed, {evaluation['test_results']['failed']} failed\n",
    "Errors: {evaluation['test_results']['errors']}\n",
    "\n",
    "Give 2-3 specific suggestions for improvement. Be direct and technical.\n",
    "Keep response under 100 words.\"\"\"\n",
    "\n",
    "        response = client.chat.completions.create(\n",
    "            model=\"gpt-4o-mini\",\n",
    "            messages=[{\"role\": \"user\", \"content\": prompt}],\n",
    "            temperature=0.5\n",
    "        )\n",
    "\n",
    "        return response.choices[0].message.content.strip()\n",
    "\n",
    "print(\"TechLead class defined!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The Arena\n",
    "\n",
    "The DevTeamArena orchestrates the competition between developers, including evaluation and the evolution phase."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DevTeamArena:\n",
    "    \"\"\"\n",
    "    Orchestrates the competition between developer agents.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self):\n",
    "        # Initialize developers with distinct personalities\n",
    "        # Named after computing pioneers: Ada Lovelace & Alan Turing\n",
    "        self.ada = Developer(\n",
    "            name=\"Ada\",\n",
    "            initial_prompt=\"You are a Python developer who values performance and efficiency above all. Write fast, optimized code.\"\n",
    "        )\n",
    "\n",
    "        self.turing = Developer(\n",
    "            name=\"Turing\",\n",
    "            initial_prompt=\"You are a Python developer who values clean, readable code. Write elegant, maintainable solutions.\"\n",
    "        )\n",
    "\n",
    "        self.tech_lead = TechLead()\n",
    "        self.problems = [PROBLEM_1, PROBLEM_2]\n",
    "        self.round_results = []\n",
    "\n",
    "    def display_solution(self, dev: Developer, code: str, evaluation: dict):\n",
    "        \"\"\"Pretty print a solution and its evaluation.\"\"\"\n",
    "        print(f\"\\n{dev.name.upper()}'s Solution:\")\n",
    "        print(\"-\" * 60)\n",
    "        print(code)\n",
    "        print(\"-\" * 60)\n",
    "        print(f\"Tests: {evaluation['test_results']['passed']} passed, \"\n",
    "              f\"{evaluation['test_results']['failed']} failed\")\n",
    "        if evaluation['test_results']['errors']:\n",
    "            print(f\"Errors: {evaluation['test_results']['errors'][:2]}\")\n",
    "        print(f\"\\nScore Breakdown:\")\n",
    "        for criterion, score in evaluation['scores'].items():\n",
    "            max_score = 40 if criterion == 'correctness' else (20 if criterion in ['performance', 'readability'] else 10)\n",
    "            print(f\"  {criterion}: {score}/{max_score}\")\n",
    "        print(f\"  TOTAL: {evaluation['total']}/100\")\n",
    "\n",
    "    def run_round(self, round_num: int):\n",
    "        \"\"\"Execute one round of competition.\"\"\"\n",
    "        problem = self.problems[round_num - 1]\n",
    "\n",
    "        print(\"\\n\" + \"=\" * 70)\n",
    "        print(f\"ROUND {round_num}: {'BASELINE' if round_num == 1 else 'EVOLVED'}\")\n",
    "        print(\"=\" * 70)\n",
    "        print(f\"\\nProblem: {problem['name']}\")\n",
    "        print(\"-\" * 70)\n",
    "\n",
    "        # Get solutions\n",
    "        print(\"\\nGenerating solutions...\")\n",
    "        ada_code = self.ada.solve(problem)\n",
    "        turing_code = self.turing.solve(problem)\n",
    "\n",
    "        # Evaluate solutions\n",
    "        print(\"Evaluating solutions...\")\n",
    "        ada_eval = self.tech_lead.evaluate(ada_code, problem)\n",
    "        turing_eval = self.tech_lead.evaluate(turing_code, problem)\n",
    "\n",
    "        # Display results\n",
    "        self.display_solution(self.ada, ada_code, ada_eval)\n",
    "        self.display_solution(self.turing, turing_code, turing_eval)\n",
    "\n",
    "        # Determine winner\n",
    "        if ada_eval['total'] > turing_eval['total']:\n",
    "            winner = self.ada\n",
    "            winner_code = ada_code\n",
    "            loser = self.turing\n",
    "        elif turing_eval['total'] > ada_eval['total']:\n",
    "            winner = self.turing\n",
    "            winner_code = turing_code\n",
    "            loser = self.ada\n",
    "        else:\n",
    "            winner = None\n",
    "            winner_code = None\n",
    "\n",
    "        print(\"\\n\" + \"-\" * 70)\n",
    "        if winner:\n",
    "            print(f\"WINNER: {winner.name} ({ada_eval['total']} vs {turing_eval['total']})\")\n",
    "        else:\n",
    "            print(f\"TIE! ({ada_eval['total']} vs {turing_eval['total']})\")\n",
    "\n",
    "        # Generate feedback\n",
    "        print(\"\\nTech Lead Feedback:\")\n",
    "        ada_feedback = self.tech_lead.generate_feedback(\"Ada\", ada_eval, ada_code)\n",
    "        turing_feedback = self.tech_lead.generate_feedback(\"Turing\", turing_eval, turing_code)\n",
    "\n",
    "        print(f\"\\nTo Ada: {ada_feedback}\")\n",
    "        print(f\"\\nTo Turing: {turing_feedback}\")\n",
    "\n",
    "        # Store results\n",
    "        self.round_results.append({\n",
    "            \"round\": round_num,\n",
    "            \"ada\": {\"code\": ada_code, \"eval\": ada_eval, \"feedback\": ada_feedback},\n",
    "            \"turing\": {\"code\": turing_code, \"eval\": turing_eval, \"feedback\": turing_feedback},\n",
    "            \"winner\": winner.name if winner else \"Tie\"\n",
    "        })\n",
    "\n",
    "        # Record scores\n",
    "        self.ada.score_history.append(ada_eval['total'])\n",
    "        self.turing.score_history.append(turing_eval['total'])\n",
    "\n",
    "        return {\n",
    "            \"ada\": {\"eval\": ada_eval, \"feedback\": ada_feedback, \"code\": ada_code},\n",
    "            \"turing\": {\"eval\": turing_eval, \"feedback\": turing_feedback, \"code\": turing_code},\n",
    "            \"winner_code\": winner_code\n",
    "        }\n",
    "\n",
    "    def run_evolution(self, round_results: dict):\n",
    "        \"\"\"Have both agents evolve their prompts based on feedback.\"\"\"\n",
    "        print(\"\\n\" + \"=\" * 70)\n",
    "        print(\"PROMPT EVOLUTION PHASE\")\n",
    "        print(\"=\" * 70)\n",
    "\n",
    "        print(\"\\nAgents are reflecting on their performance...\")\n",
    "\n",
    "        # Ada evolves\n",
    "        ada_new = self.ada.evolve_prompt(\n",
    "            feedback=round_results[\"ada\"][\"feedback\"],\n",
    "            my_score=round_results[\"ada\"][\"eval\"][\"total\"],\n",
    "            opponent_score=round_results[\"turing\"][\"eval\"][\"total\"],\n",
    "            winning_code=round_results[\"winner_code\"] if round_results[\"turing\"][\"eval\"][\"total\"] > round_results[\"ada\"][\"eval\"][\"total\"] else None\n",
    "        )\n",
    "\n",
    "        # Turing evolves\n",
    "        turing_new = self.turing.evolve_prompt(\n",
    "            feedback=round_results[\"turing\"][\"feedback\"],\n",
    "            my_score=round_results[\"turing\"][\"eval\"][\"total\"],\n",
    "            opponent_score=round_results[\"ada\"][\"eval\"][\"total\"],\n",
    "            winning_code=round_results[\"winner_code\"] if round_results[\"ada\"][\"eval\"][\"total\"] > round_results[\"turing\"][\"eval\"][\"total\"] else None\n",
    "        )\n",
    "\n",
    "        print(\"\\n\" + \"-\" * 70)\n",
    "        print(\"ADA's Prompt Evolution:\")\n",
    "        print(\"-\" * 70)\n",
    "        print(f\"OLD: {self.ada.prompt_history[0]}\")\n",
    "        print(f\"\\nNEW: {ada_new}\")\n",
    "\n",
    "        print(\"\\n\" + \"-\" * 70)\n",
    "        print(\"TURING's Prompt Evolution:\")\n",
    "        print(\"-\" * 70)\n",
    "        print(f\"OLD: {self.turing.prompt_history[0]}\")\n",
    "        print(f\"\\nNEW: {turing_new}\")\n",
    "\n",
    "    def show_summary(self):\n",
    "        \"\"\"Display the evolution summary.\"\"\"\n",
    "        print(\"\\n\" + \"=\" * 70)\n",
    "        print(\"EVOLUTION SUMMARY\")\n",
    "        print(\"=\" * 70)\n",
    "\n",
    "        print(\"\\nScore Progression:\")\n",
    "        print(\"-\" * 40)\n",
    "        print(f\"{'':15} {'Round 1':>12} {'Round 2':>12} {'Change':>12}\")\n",
    "        print(\"-\" * 40)\n",
    "\n",
    "        ada_change = self.ada.score_history[1] - self.ada.score_history[0] if len(self.ada.score_history) > 1 else 0\n",
    "        turing_change = self.turing.score_history[1] - self.turing.score_history[0] if len(self.turing.score_history) > 1 else 0\n",
    "\n",
    "        ada_change_str = f\"+{ada_change}\" if ada_change > 0 else str(ada_change)\n",
    "        turing_change_str = f\"+{turing_change}\" if turing_change > 0 else str(turing_change)\n",
    "\n",
    "        print(f\"{'Ada':15} {self.ada.score_history[0]:>12} {self.ada.score_history[1] if len(self.ada.score_history) > 1 else 'N/A':>12} {ada_change_str:>12}\")\n",
    "        print(f\"{'Turing':15} {self.turing.score_history[0]:>12} {self.turing.score_history[1] if len(self.turing.score_history) > 1 else 'N/A':>12} {turing_change_str:>12}\")\n",
    "\n",
    "        print(\"\\n\" + \"-\" * 70)\n",
    "        print(\"Key Observations:\")\n",
    "\n",
    "        if ada_change > 0 and turing_change > 0:\n",
    "            print(\"- Both agents improved after self-reflection!\")\n",
    "        elif ada_change > turing_change:\n",
    "            print(\"- Ada showed more improvement after evolution\")\n",
    "        elif turing_change > ada_change:\n",
    "            print(\"- Turing showed more improvement after evolution\")\n",
    "\n",
    "        bigger_improver = \"Ada\" if ada_change > turing_change else \"Turing\"\n",
    "        if abs(ada_change - turing_change) > 5:\n",
    "            print(f\"- {bigger_improver} learned more from the feedback\")\n",
    "\n",
    "        print(\"\\nPrompt Evolution Highlights:\")\n",
    "        print(f\"- Ada went from {len(self.ada.prompt_history[0])} to {len(self.ada.prompt_history[-1])} chars\")\n",
    "        print(f\"- Turing went from {len(self.turing.prompt_history[0])} to {len(self.turing.prompt_history[-1])} chars\")\n",
    "\n",
    "print(\"DevTeamArena class defined!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Round 1: Baseline Performance\n",
    "\n",
    "Let's run the first round with the agents' initial prompts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the arena\n",
    "arena = DevTeamArena()\n",
    "\n",
    "# Run Round 1\n",
    "round1_results = arena.run_round(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evolution Phase\n",
    "\n",
    "Now each agent reflects on their performance and rewrites their system prompt. This is the key innovation: **the agents modify their own instructions** based on what they learned."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run the evolution phase\n",
    "arena.run_evolution(round1_results)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Round 2: Evolved Performance\n",
    "\n",
    "Now let's test the evolved prompts on a new problem. Did the self-reflection help?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run Round 2 with evolved prompts\n",
    "round2_results = arena.run_round(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Final Summary\n",
    "\n",
    "Let's compare how the agents performed before and after evolution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show the evolution summary\n",
    "arena.show_summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Viewing the Full Prompt History\n",
    "\n",
    "Let's see the complete evolution of each agent's system prompt."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"COMPLETE PROMPT HISTORY\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "print(\"\\nADA's Journey:\")\n",
    "print(\"-\" * 70)\n",
    "for i, prompt in enumerate(arena.ada.prompt_history):\n",
    "    label = \"Initial\" if i == 0 else f\"After Round {i}\"\n",
    "    score = arena.ada.score_history[i] if i < len(arena.ada.score_history) else \"N/A\"\n",
    "    print(f\"\\n[{label}] (Score: {score}/100)\")\n",
    "    print(prompt)\n",
    "\n",
    "print(\"\\n\" + \"=\" * 70)\n",
    "print(\"\\nTURING's Journey:\")\n",
    "print(\"-\" * 70)\n",
    "for i, prompt in enumerate(arena.turing.prompt_history):\n",
    "    label = \"Initial\" if i == 0 else f\"After Round {i}\"\n",
    "    score = arena.turing.score_history[i] if i < len(arena.turing.score_history) else \"N/A\"\n",
    "    print(f\"\\n[{label}] (Score: {score}/100)\")\n",
    "    print(prompt)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Key Takeaways\n",
    "\n",
    "### What We Observed\n",
    "\n",
    "1. **Self-Reflection Works**: Agents that analyze their mistakes can improve their own instructions.\n",
    "\n",
    "2. **Competition Drives Improvement**: The losing agent has strong incentive to learn from the winner's approach.\n",
    "\n",
    "3. **Emergent Strategies**: The evolved prompts often contain insights we didn't explicitly teach:\n",
    "   - \"Always check edge cases first\"\n",
    "   - \"Balance elegance with correctness\"\n",
    "   - \"Add type hints and docstrings\"\n",
    "\n",
    "4. **Convergence**: Despite starting with opposite philosophies, agents often converge toward similar best practices.\n",
    "\n",
    "### The Meta Pattern\n",
    "\n",
    "```\n",
    "Prompt -> Behavior -> Evaluation -> Reflection -> Better Prompt\n",
    "```\n",
    "\n",
    "This is the same pattern as:\n",
    "- Demo 1 (code -> error -> fix)\n",
    "- Demo 2 (solution -> fitness -> evolution)\n",
    "- Demo 3 (capability gap -> tool creation)\n",
    "\n",
    "But applied to the agent's own instructions!\n",
    "\n",
    "### Real-World Applications\n",
    "\n",
    "- **OPRO (Google)**: Optimizing prompts for math problems\n",
    "- **Constitutional AI**: Improving AI behavior through self-critique\n",
    "- **Automated Prompt Engineering**: Finding optimal prompts for specific tasks\n",
    "\n",
    "### Limitations\n",
    "\n",
    "- Requires good evaluation metrics\n",
    "- May overfit to specific test cases\n",
    "- Evolution is bounded by the LLM's capabilities\n",
    "- Need safeguards against \"prompt collapse\"\n",
    "\n",
    "---\n",
    "\n",
    "**Congratulations!** You've completed all four demos of self-improving code.\n",
    "\n",
    "From simple bug fixing to agents that rewrite their own instructions, we've explored the frontier of adaptive software. The future is code that learns, evolves, and grows."
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
